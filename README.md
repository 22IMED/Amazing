# ğŸ“Š Segmentation Clients avec Kedro & Snowflake

[![Powered by Kedro](https://img.shields.io/badge/powered_by-kedro-ffc900?logo=kedro)](https://kedro.org)  
[![Snowflake](https://img.shields.io/badge/Data%20Warehouse-Snowflake-blue?logo=snowflake)](https://www.snowflake.com/)  
[![Docker](https://img.shields.io/badge/Deploy-Docker-2496ED?logo=docker&logoColor=white)](https://www.docker.com/)  

---

## ğŸ“ AperÃ§u du projet

Ce projet met en place un **prototype de solution dâ€™IA** permettant de :  
- PrÃ©parer et transformer les donnÃ©es dâ€™Ã©vÃ©nements clients (**ETL Kedro**)  
- Construire des indicateurs **RFM enrichis** (Recency, Frequency, Monetary + comportements)  
- Appliquer un modÃ¨le de **clustering KMeans** pour classer les clients selon leur profil  
- Stocker les rÃ©sultats (clients classÃ©s) dans **Snowflake** via **Snowpark**  
- Fournir une pipeline prÃªte Ã  Ãªtre packagÃ©e dans **Docker** pour un dÃ©ploiement en production  

---

## âš™ï¸ Architecture du projet

Flux de traitement :  

**Ã‰vÃ©nements clients â†’ ETL Kedro â†’ PrÃ©paration RFM â†’ ModÃ¨le KMeans â†’ RÃ©sultats dans Snowflake**

Les pipelines sont organisÃ©s en *nodes* modulaires :  

- `prepare_rfm_node` â†’ PrÃ©paration des features  
- `train_model_node` â†’ EntraÃ®nement et Ã©valuation (inertia, silhouette)  
- `predict_new_clients_node` â†’ Classification des nouveaux clients  
- `store_results_node` â†’ Sauvegarde des rÃ©sultats dans Snowflake  

Exemple dâ€™architecture (Mermaid) :  

```mermaid
flowchart LR
    A[Snowflake Events] --> B[ETL Kedro]
    B --> C[Features RFM]
    C --> D[ModÃ¨le KMeans]
    D --> E[Clients ClassÃ©s]
    E --> F[(Table Snowflake)]
```

---

## ğŸš€ Installation

Cloner le projet et installer les dÃ©pendances :  

```bash
git clone <url-du-repo>
cd mspr2
pip install -r requirements.txt
```

Configurer vos accÃ¨s **Snowflake** dans `conf/local/credentials.yml`  

export SNOWFLAKE_USER=xxx
export SNOWFLAKE_PASSWORD=xxx
export SNOWFLAKE_ACCOUNT=xxx
export SNOWFLAKE_WAREHOUSE=xxx
export SNOWFLAKE_DATABASE=AMAZING_DB
export SNOWFLAKE_SCHEMA=SCHEMAS


âš ï¸ Ne jamais commiter vos credentials dans le repository.

---

## â–¶ï¸ ExÃ©cution des pipelines

Lancer lâ€™ETL et le pipeline de classification :  

```bash
kedro run
```

Exemple pour lancer uniquement la prÃ©diction de nouveaux clients :  

```bash
kedro run --pipeline prediction
```

---


## ğŸ³ DÃ©ploiement avec Docker

Une image Docker est disponible pour exÃ©cuter le pipeline dans un environnement isolÃ©.  

Construire lâ€™image :  
```bash
docker build -t mspr2:latest .
```

Lancer le conteneur sur le cloudâ€¯:  
```bash
docker run --rm -it \
  -e SNOWFLAKE_USER=xxx \
  -e SNOWFLAKE_PASSWORD=xxx \
  -e SNOWFLAKE_ACCOUNT=xxx \
  -e SNOWFLAKE_WAREHOUSE=xxx \
  -e SNOWFLAKE_DATABASE=AMAZING_DB \
  -e SNOWFLAKE_SCHEMA=SCHEMAS \
  mspr2-prediction:latest \
  kedro run --pipeline=prediction_pipeline

```

---

## ğŸ“‚ Structure du projet

```
mspr2/
â”œâ”€â”€ conf/                 # Configuration Kedro (credentials, catalogues, paramsâ€¦)
â”œâ”€â”€ data/                 # DonnÃ©es locales (jamais commit)
â”œâ”€â”€ src/mspr2/            # Code source Kedro (pipelines, nodes, hooksâ€¦)
â”œâ”€â”€ tests/                # Tests unitaires
â”œâ”€â”€ requirements.txt      # DÃ©pendances Python
â””â”€â”€ README.md             # Ce fichier
```

---

## ğŸ“Š RÃ©sultats attendus

- Segmentation automatique des clients en **clusters** basÃ©s sur leur comportement  
- Table Snowflake mise Ã  jour contenant les clients classÃ©s  
- Pipelines modulaires pour sâ€™intÃ©grer facilement dans un environnement de production  

---

## ğŸ“š RÃ©fÃ©rences

- [Documentation Kedro](https://docs.kedro.org)  
- [Snowflake Snowpark](https://docs.snowflake.com/en/developer-guide/snowpark/python/index)  
- [Scikit-Learn Clustering](https://scikit-learn.org/stable/modules/clustering.html)  

---
